{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "L3JHhFK-KM61",
        "outputId": "cfea9a6a-1c00-476c-f2c7-1acc75ec209e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlakshaybaijal\u001b[0m (\u001b[33mlakshaybaijal-iiit-hyderabad\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['angry', 'happy', 'sad']\n",
            "Sizes ▶ Train: 624 Val: 133 Test: 135\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os, glob\n",
        "from PIL import Image\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision import transforms, models\n",
        "from torchvision.datasets import ImageFolder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "from scipy.stats import zscore\n",
        "import wandb\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "num_epochs    = 10\n",
        "batch_size    = 32\n",
        "learning_rate = 1e-4\n",
        "emo_dir       = '/content/drive/MyDrive/smai_a2/dataset/emotions'  # ← update as needed\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "val_tfms = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "full_emo_ds = ImageFolder(emo_dir, transform=train_tfms)\n",
        "n = len(full_emo_ds)\n",
        "n_train = int(0.7*n)\n",
        "n_val   = int(0.15*n)\n",
        "n_test  = n - n_train - n_val\n",
        "train_ds, val_ds, test_ds = random_split(full_emo_ds, [n_train, n_val, n_test])\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=2)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"Classes:\", full_emo_ds.classes)\n",
        "print(\"Sizes ▶ Train:\", len(train_ds), \"Val:\", len(val_ds), \"Test:\", len(test_ds))\n",
        "\n",
        "def train_epoch(model, loader, opt, crit):\n",
        "    model.train(); total_loss=0; correct=0; total=0\n",
        "    for imgs, lbls in loader:\n",
        "        imgs, lbls = imgs.to(device), lbls.to(device)\n",
        "        opt.zero_grad()\n",
        "        out = model(imgs)\n",
        "        loss = crit(out, lbls)\n",
        "        loss.backward(); opt.step()\n",
        "        total_loss += loss.item()*imgs.size(0)\n",
        "        preds = out.argmax(1)\n",
        "        correct += (preds==lbls).sum().item()\n",
        "        total += lbls.size(0)\n",
        "    return total_loss/total, correct/total\n",
        "\n",
        "def eval_epoch(model, loader, crit):\n",
        "    model.eval(); total_loss=0; correct=0; total=0\n",
        "    with torch.no_grad():\n",
        "        for imgs, lbls in loader:\n",
        "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
        "            out = model(imgs)\n",
        "            loss = crit(out, lbls)\n",
        "            total_loss += loss.item()*imgs.size(0)\n",
        "            preds = out.argmax(1)\n",
        "            correct += (preds==lbls).sum().item()\n",
        "            total += lbls.size(0)\n",
        "    return total_loss/total, correct/total\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(\n",
        "  project=\"smai-assignment2\",\n",
        "  name=\"emo_vgg16\",\n",
        "  config={\n",
        "      \"model\": \"VGG16-finetune\",\n",
        "      \"epochs\": num_epochs,\n",
        "      \"batch_size\": batch_size,\n",
        "      \"lr\": learning_rate\n",
        "  }\n",
        ")\n",
        "cfg = wandb.config\n",
        "\n",
        "model_vgg = models.vgg16(pretrained=True)\n",
        "for p in model_vgg.features.parameters(): p.requires_grad=False\n",
        "model_vgg.classifier[6] = nn.Linear(model_vgg.classifier[6].in_features, len(full_emo_ds.classes))\n",
        "model_vgg = model_vgg.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_vgg.classifier[6].parameters(), lr=cfg.lr)\n",
        "\n",
        "wandb.watch(model_vgg, log=\"all\", log_freq=10)\n",
        "\n",
        "for ep in range(cfg.epochs):\n",
        "    tl, ta = train_epoch(model_vgg, train_loader, optimizer, criterion)\n",
        "    vl, va = eval_epoch (model_vgg, val_loader,   criterion)\n",
        "    print(f\"[VGG Lakshay] Epoch {ep+1}/{cfg.epochs} ▶ Train: {ta:.4f}, Val: {va:.4f}\")\n",
        "    wandb.log({\n",
        "      \"epoch\": ep+1,\n",
        "      \"train/loss\": tl, \"train/acc\": ta,\n",
        "      \"val/loss\": vl,   \"val/acc\": va\n",
        "    })\n",
        "\n",
        "y_true=y_pred=[]\n",
        "y_true, y_pred = [], []\n",
        "model_vgg.eval()\n",
        "with torch.no_grad():\n",
        "    for imgs, lbls in test_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        out  = model_vgg(imgs)\n",
        "        preds = out.argmax(1).cpu().numpy()\n",
        "        y_pred.extend(preds); y_true.extend(lbls.numpy())\n",
        "\n",
        "wandb.log({\n",
        "  \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
        "      probs=None, y_true=y_true, preds=y_pred, class_names=full_emo_ds.classes\n",
        "  )\n",
        "})\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, target_names=full_emo_ds.classes))\n",
        "\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OjfpufRML3ID",
        "outputId": "cfcfcc24-a582-4166-f448-a9723ec2d9ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250423_180607-np8b6ifd</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2/runs/np8b6ifd' target=\"_blank\">emo_vgg16</a></strong> to <a href='https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2' target=\"_blank\">https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2/runs/np8b6ifd' target=\"_blank\">https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2/runs/np8b6ifd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:03<00:00, 160MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VGG Lakshay] Epoch 1/10 ▶ Train: 0.3221, Val: 0.3383\n",
            "[VGG Lakshay] Epoch 2/10 ▶ Train: 0.3478, Val: 0.3534\n",
            "[VGG Lakshay] Epoch 3/10 ▶ Train: 0.3670, Val: 0.3985\n",
            "[VGG Lakshay] Epoch 4/10 ▶ Train: 0.3958, Val: 0.4511\n",
            "[VGG Lakshay] Epoch 5/10 ▶ Train: 0.4135, Val: 0.5038\n",
            "[VGG Lakshay] Epoch 6/10 ▶ Train: 0.4439, Val: 0.4662\n",
            "[VGG Lakshay] Epoch 7/10 ▶ Train: 0.4824, Val: 0.5263\n",
            "[VGG Lakshay] Epoch 8/10 ▶ Train: 0.4647, Val: 0.5489\n",
            "[VGG Lakshay] Epoch 9/10 ▶ Train: 0.5529, Val: 0.5564\n",
            "[VGG Lakshay] Epoch 10/10 ▶ Train: 0.5321, Val: 0.6391\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       angry       0.91      0.56      0.69        52\n",
            "       happy       0.64      0.53      0.58        43\n",
            "         sad       0.52      0.88      0.65        40\n",
            "\n",
            "    accuracy                           0.64       135\n",
            "   macro avg       0.69      0.66      0.64       135\n",
            "weighted avg       0.71      0.64      0.65       135\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train/acc</td><td>▁▂▂▃▄▅▆▅█▇</td></tr><tr><td>train/loss</td><td>█▇▇▆▅▄▃▃▁▁</td></tr><tr><td>val/acc</td><td>▁▁▂▄▅▄▅▆▆█</td></tr><tr><td>val/loss</td><td>██▇▆▄▅▅▃▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train/acc</td><td>0.53205</td></tr><tr><td>train/loss</td><td>0.98548</td></tr><tr><td>val/acc</td><td>0.6391</td></tr><tr><td>val/loss</td><td>0.96341</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">emo_vgg16</strong> at: <a href='https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2/runs/np8b6ifd' target=\"_blank\">https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2/runs/np8b6ifd</a><br> View project at: <a href='https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2' target=\"_blank\">https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250423_180607-np8b6ifd/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(\n",
        "  project=\"smai-assignment2\",\n",
        "  name=\"emo_resnet18_scratch\",\n",
        "  config={\n",
        "      \"model\": \"ResNet18-scratch\",\n",
        "      \"epochs\": num_epochs,\n",
        "      \"batch_size\": batch_size,\n",
        "      \"lr\": learning_rate\n",
        "  }\n",
        ")\n",
        "cfg = wandb.config\n",
        "\n",
        "model_rs = models.resnet18(pretrained=False)\n",
        "model_rs.fc = nn.Linear(model_rs.fc.in_features, len(full_emo_ds.classes))\n",
        "model_rs = model_rs.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_rs.parameters(), lr=cfg.lr)\n",
        "\n",
        "wandb.watch(model_rs, log=\"all\", log_freq=10)\n",
        "\n",
        "\n",
        "for ep in range(cfg.epochs):\n",
        "    tl, ta = train_epoch(model_rs, train_loader, optimizer, criterion)\n",
        "    vl, va = eval_epoch (model_rs, val_loader,   criterion)\n",
        "    print(f\"[ResNet18 not pretrained Lakshay] Epoch {ep+1}/{cfg.epochs} ▶ Train: {ta:.4f}, Val: {va:.4f}\")\n",
        "    wandb.log({\n",
        "      \"epoch\": ep+1,\n",
        "      \"train/loss\": tl, \"train/acc\": ta,\n",
        "      \"val/loss\": vl,   \"val/acc\": va\n",
        "    })\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "model_rs.eval()\n",
        "with torch.no_grad():\n",
        "    for imgs, lbls in test_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        out  = model_rs(imgs)\n",
        "        preds = out.argmax(1).cpu().numpy()\n",
        "        y_pred.extend(preds); y_true.extend(lbls.numpy())\n",
        "\n",
        "wandb.log({\n",
        "  \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
        "      probs=None, y_true=y_true, preds=y_pred, class_names=full_emo_ds.classes\n",
        "  )\n",
        "})\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, target_names=full_emo_ds.classes))\n",
        "\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FDMLawJkL66Z",
        "outputId": "20d2246f-1b90-4736-bf5f-aeddf22ab77a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250423_181131-wgadfc6q</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2/runs/wgadfc6q' target=\"_blank\">emo_resnet18_scratch</a></strong> to <a href='https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2' target=\"_blank\">https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2/runs/wgadfc6q' target=\"_blank\">https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2/runs/wgadfc6q</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ResNet18 not pretrained Lakshay] Epoch 1/10 ▶ Train: 0.3862, Val: 0.3233\n",
            "[ResNet18 not pretrained Lakshay] Epoch 2/10 ▶ Train: 0.5369, Val: 0.4887\n",
            "[ResNet18 not pretrained Lakshay] Epoch 3/10 ▶ Train: 0.6218, Val: 0.3684\n",
            "[ResNet18 not pretrained Lakshay] Epoch 4/10 ▶ Train: 0.7324, Val: 0.6391\n",
            "[ResNet18 not pretrained Lakshay] Epoch 5/10 ▶ Train: 0.7500, Val: 0.7368\n",
            "[ResNet18 not pretrained Lakshay] Epoch 6/10 ▶ Train: 0.8189, Val: 0.7970\n",
            "[ResNet18 not pretrained Lakshay] Epoch 7/10 ▶ Train: 0.8926, Val: 0.7820\n",
            "[ResNet18 not pretrained Lakshay] Epoch 8/10 ▶ Train: 0.8462, Val: 0.8421\n",
            "[ResNet18 not pretrained Lakshay] Epoch 9/10 ▶ Train: 0.8814, Val: 0.7820\n",
            "[ResNet18 not pretrained Lakshay] Epoch 10/10 ▶ Train: 0.9295, Val: 0.8346\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       angry       0.90      0.73      0.81        52\n",
            "       happy       0.97      0.74      0.84        43\n",
            "         sad       0.63      0.95      0.76        40\n",
            "\n",
            "    accuracy                           0.80       135\n",
            "   macro avg       0.84      0.81      0.80       135\n",
            "weighted avg       0.85      0.80      0.80       135\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train/acc</td><td>▁▃▄▅▆▇█▇▇█</td></tr><tr><td>train/loss</td><td>█▆▅▄▃▃▂▂▂▁</td></tr><tr><td>val/acc</td><td>▁▃▂▅▇▇▇█▇█</td></tr><tr><td>val/loss</td><td>█▅▇▄▃▁▂▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train/acc</td><td>0.92949</td></tr><tr><td>train/loss</td><td>0.21172</td></tr><tr><td>val/acc</td><td>0.83459</td></tr><tr><td>val/loss</td><td>0.42278</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">emo_resnet18_scratch</strong> at: <a href='https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2/runs/wgadfc6q' target=\"_blank\">https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2/runs/wgadfc6q</a><br> View project at: <a href='https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2' target=\"_blank\">https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250423_181131-wgadfc6q/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ▶ ResNet18 Pretrained\n",
        "wandb.init(\n",
        "  project=\"smai-assignment2\",\n",
        "  name=\"emo_resnet18_pretrained\",\n",
        "  config={\n",
        "      \"model\": \"ResNet18-pretrained\",\n",
        "      \"epochs\": num_epochs,\n",
        "      \"batch_size\": batch_size,\n",
        "      \"lr\": learning_rate\n",
        "  }\n",
        ")\n",
        "cfg = wandb.config\n",
        "\n",
        "# Model\n",
        "model_rp = models.resnet18(pretrained=True)\n",
        "model_rp.fc = nn.Linear(model_rp.fc.in_features, len(full_emo_ds.classes))\n",
        "model_rp = model_rp.to(device)\n",
        "\n",
        "# Opt & Loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_rp.parameters(), lr=cfg.lr)\n",
        "\n",
        "# Watch\n",
        "wandb.watch(model_rp, log=\"all\", log_freq=10)\n",
        "\n",
        "# Train\n",
        "for ep in range(cfg.epochs):\n",
        "    tl, ta = train_epoch(model_rp, train_loader, optimizer, criterion)\n",
        "    vl, va = eval_epoch (model_rp, val_loader,   criterion)\n",
        "    print(f\"[ResNet18 Pretrained Lakshay] Epoch {ep+1}/{cfg.epochs} ▶ Train: {ta:.4f}, Val: {va:.4f}\")\n",
        "    wandb.log({\n",
        "      \"epoch\": ep+1,\n",
        "      \"train/loss\": tl, \"train/acc\": ta,\n",
        "      \"val/loss\": vl,   \"val/acc\": va\n",
        "    })\n",
        "\n",
        "# Confusion Matrix on Test\n",
        "y_true, y_pred = [], []\n",
        "model_rp.eval()\n",
        "with torch.no_grad():\n",
        "    for imgs, lbls in test_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        out  = model_rp(imgs)\n",
        "        preds = out.argmax(1).cpu().numpy()\n",
        "        y_pred.extend(preds); y_true.extend(lbls.numpy())\n",
        "\n",
        "wandb.log({\n",
        "  \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
        "      probs=None, y_true=y_true, preds=y_pred, class_names=full_emo_ds.classes\n",
        "  )\n",
        "})\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, target_names=full_emo_ds.classes))\n",
        "\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "933fQEvNL8j7",
        "outputId": "44f77802-d052-464d-d25b-82b4eb1194c0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250423_181259-yq0h7sua</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2/runs/yq0h7sua' target=\"_blank\">emo_resnet18_pretrained</a></strong> to <a href='https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2' target=\"_blank\">https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2/runs/yq0h7sua' target=\"_blank\">https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2/runs/yq0h7sua</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 171MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ResNet18 Pretrained Lakshay] Epoch 1/10 ▶ Train: 0.7003, Val: 0.9023\n",
            "[ResNet18 Pretrained Lakshay] Epoch 2/10 ▶ Train: 0.9712, Val: 0.9699\n",
            "[ResNet18 Pretrained Lakshay] Epoch 3/10 ▶ Train: 0.9888, Val: 0.9699\n",
            "[ResNet18 Pretrained Lakshay] Epoch 4/10 ▶ Train: 0.9888, Val: 0.9850\n",
            "[ResNet18 Pretrained Lakshay] Epoch 5/10 ▶ Train: 0.9888, Val: 0.9774\n",
            "[ResNet18 Pretrained Lakshay] Epoch 6/10 ▶ Train: 0.9936, Val: 0.9925\n",
            "[ResNet18 Pretrained Lakshay] Epoch 7/10 ▶ Train: 0.9920, Val: 1.0000\n",
            "[ResNet18 Pretrained Lakshay] Epoch 8/10 ▶ Train: 1.0000, Val: 0.9925\n",
            "[ResNet18 Pretrained Lakshay] Epoch 9/10 ▶ Train: 1.0000, Val: 0.9850\n",
            "[ResNet18 Pretrained Lakshay] Epoch 10/10 ▶ Train: 0.9984, Val: 1.0000\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       angry       1.00      1.00      1.00        52\n",
            "       happy       1.00      1.00      1.00        43\n",
            "         sad       1.00      1.00      1.00        40\n",
            "\n",
            "    accuracy                           1.00       135\n",
            "   macro avg       1.00      1.00      1.00       135\n",
            "weighted avg       1.00      1.00      1.00       135\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train/acc</td><td>▁▇████████</td></tr><tr><td>train/loss</td><td>█▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▆▆▇▆▇█▇▇█</td></tr><tr><td>val/loss</td><td>█▃▄▂▂▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train/acc</td><td>0.9984</td></tr><tr><td>train/loss</td><td>0.00614</td></tr><tr><td>val/acc</td><td>1</td></tr><tr><td>val/loss</td><td>0.00673</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">emo_resnet18_pretrained</strong> at: <a href='https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2/runs/yq0h7sua' target=\"_blank\">https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2/runs/yq0h7sua</a><br> View project at: <a href='https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2' target=\"_blank\">https://wandb.ai/lakshaybaijal-iiit-hyderabad/smai-assignment2</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250423_181259-yq0h7sua/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Cell X: Build fresh “clean” emotion models (no W&B hooks)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "# 1) Fresh VGG16 for 3-class emotions\n",
        "vgg_emo_clean = models.vgg16(pretrained=True)\n",
        "vgg_emo_clean.classifier[6] = nn.Linear(\n",
        "    vgg_emo_clean.classifier[6].in_features,\n",
        "    len(full_emo_ds.classes)\n",
        ")\n",
        "vgg_emo_clean.load_state_dict(model_vgg.state_dict())\n",
        "vgg_emo_clean = vgg_emo_clean.to(device)\n",
        "\n",
        "# 2) Fresh ResNet18 (from scratch)\n",
        "rs_emo_clean = models.resnet18(pretrained=False)\n",
        "rs_emo_clean.fc = nn.Linear(rs_emo_clean.fc.in_features,\n",
        "                            len(full_emo_ds.classes))\n",
        "rs_emo_clean.load_state_dict(model_rs.state_dict())\n",
        "rs_emo_clean = rs_emo_clean.to(device)\n",
        "\n",
        "# 3) Fresh ResNet18 (pretrained)\n",
        "rp_emo_clean = models.resnet18(pretrained=True)\n",
        "rp_emo_clean.fc = nn.Linear(rp_emo_clean.fc.in_features,\n",
        "                            len(full_emo_ds.classes))\n",
        "rp_emo_clean.load_state_dict(model_rp.state_dict())\n",
        "rp_emo_clean = rp_emo_clean.to(device)\n",
        "\n",
        "print(\"Clean emotion models ready:\",\n",
        "      vgg_emo_clean, rs_emo_clean, rp_emo_clean)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B29cFpwIzl-s",
        "outputId": "f1d50402-c74d-49da-df6d-047105d59001"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean emotion models ready: VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=3, bias=True)\n",
            "  )\n",
            ") ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
            ") ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Cell Y: Generate the 3 Emotion-Recognition videos\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "import glob, cv2\n",
        "from PIL import Image\n",
        "\n",
        "def make_emotion_video(\n",
        "    model, class_names, img_paths, output_path,\n",
        "    transform, device, fps=1, frame_size=(224,224)\n",
        "):\n",
        "    model.eval()\n",
        "    vw = cv2.VideoWriter(\n",
        "        output_path,\n",
        "        cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "        fps, frame_size\n",
        "    )\n",
        "    for path in img_paths:\n",
        "        img_pil = Image.open(path).convert('RGB')\n",
        "        inp     = transform(img_pil).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(inp).argmax(1).item()\n",
        "        lbl   = class_names[pred]\n",
        "        frame = cv2.resize(cv2.imread(path), frame_size)\n",
        "        cv2.putText(frame, lbl, (10,frame_size[1]-10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1.2,\n",
        "                    (0,255,0), 2, cv2.LINE_AA)\n",
        "        vw.write(frame)\n",
        "    vw.release()\n",
        "    print(f\"✅ Saved video: {output_path}\")\n",
        "\n",
        "# Gather file-paths for your emotion test split:\n",
        "emo_test_paths = [ full_emo_ds.samples[i][0] for i in test_ds.indices ]\n",
        "\n",
        "# 1) VGG16 emotion\n",
        "make_emotion_video(\n",
        "    vgg_emo_clean,\n",
        "    full_emo_ds.classes,\n",
        "    emo_test_paths,\n",
        "    'vgg_emotion_recognition.mp4',\n",
        "    val_tfms,\n",
        "    device\n",
        ")\n",
        "\n",
        "# 2) ResNet18 pretrained emotion\n",
        "make_emotion_video(\n",
        "    rp_emo_clean,\n",
        "    full_emo_ds.classes,\n",
        "    emo_test_paths,\n",
        "    'resnet18_pretrained_emotion.mp4',\n",
        "    val_tfms,\n",
        "    device\n",
        ")\n",
        "\n",
        "# 3) ResNet18 scratch emotion\n",
        "make_emotion_video(\n",
        "    rs_emo_clean,\n",
        "    full_emo_ds.classes,\n",
        "    emo_test_paths,\n",
        "    'resnet18_scratch_emotion.mp4',\n",
        "    val_tfms,\n",
        "    device\n",
        ")\n",
        "\n",
        "# (Optional) Download the videos\n",
        "from google.colab import files\n",
        "files.download('vgg_emotion_recognition.mp4')\n",
        "files.download('resnet18_pretrained_emotion.mp4')\n",
        "files.download('resnet18_scratch_emotion.mp4')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "Qk7IYH8Hzqgy",
        "outputId": "9851b928-bcc7-4d3e-af9b-06d829e50127"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved video: vgg_emotion_recognition.mp4\n",
            "✅ Saved video: resnet18_pretrained_emotion.mp4\n",
            "✅ Saved video: resnet18_scratch_emotion.mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e0718f9c-f4c1-4080-8c6a-a3e2dba40a61\", \"vgg_emotion_recognition.mp4\", 824293)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_21a29fa6-715f-497a-a9d4-78924c67daa8\", \"resnet18_pretrained_emotion.mp4\", 836794)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0c3df2cc-d3bc-46f2-9e69-1457519a7edb\", \"resnet18_scratch_emotion.mp4\", 815719)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qnqsrytUzp-h"
      }
    }
  ]
}